import time
import csv
import json
import os
import argparse
import logging
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.safari.service import Service as SafariService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select, WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Import email notifier
try:
    from email_notifier import EmailNotifier
    EMAIL_AVAILABLE = True
except ImportError:
    EMAIL_AVAILABLE = False
    print("Email notification not available. Install Gmail API dependencies.")

# Option 1: Safari WebDriver (macOS built-in)
# Option 2: Chrome WebDriver with proper ARM64 driver
CHROMEDRIVER_PATH = "/Users/jace/Desktop/nick traffic/chromedriver-mac-arm64/chromedriver"  # Correct ARM64 driver
USE_SAFARI = False  # Set to True to use Safari (requires Safari setup), False for Chrome

logging.basicConfig(filename='chp_scraper_debug.log', 
                    level=logging.INFO, 
                    format='%(asctime)s | %(levelname)s | %(message)s')

URL = "https://cad.chp.ca.gov/Traffic.aspx"

# Global variable to store previous incidents for comparison
previous_incidents = None

# Configuration
COMMUNICATION_CENTERS = {
    'BCCC': 'Border',
    'CCC': 'Central', 
    'NCCC': 'Northern',
    'SCCC': 'Southern'
}

def compare_incidents(old_incidents, new_incidents):
    """Compare two sets of incidents and return differences"""
    if old_incidents is None:
        return {"new_incidents": new_incidents, "removed_incidents": [], "changed_incidents": []}
    
    # Convert to sets of tuples for comparison (excluding Details column which is always "Details")
    old_set = set(tuple(row[1:]) for row in old_incidents)  # Skip Details column
    new_set = set(tuple(row[1:]) for row in new_incidents)
    
    # Find differences
    new_incidents_list = [row for row in new_incidents if tuple(row[1:]) in new_set - old_set]
    removed_incidents_list = [row for row in old_incidents if tuple(row[1:]) in old_set - new_set]
    
    return {
        "new_incidents": new_incidents_list,
        "removed_incidents": removed_incidents_list,
        "changed_incidents": []  # For now, we'll treat all changes as new/removed
    }

def print_incident_summary(incidents, title):
    """Print a summary of incidents"""
    if not incidents:
        return
    
    print(f"\nğŸ“‹ {title}:")
    for incident in incidents:
        if len(incident) >= 7:
            print(f"   â€¢ #{incident[1]} | {incident[2]} | {incident[3]} | {incident[4]} | {incident[6]}")

def extract_incidents_from_table(driver):
    """Extract incident data from the incidents table"""
    try:
        # Find the incidents table
        table = driver.find_element(By.TAG_NAME, "table")
        rows = table.find_elements(By.TAG_NAME, "tr")
        
        incidents_data = []
        
        for i, row in enumerate(rows):
            cells = row.find_elements(By.TAG_NAME, "td")
            if not cells:
                # Try th elements for headers
                cells = row.find_elements(By.TAG_NAME, "th")
            
            if cells and i > 0:  # Skip header row
                cell_texts = [cell.text.strip() for cell in cells]
                if len(cell_texts) >= 7:  # Ensure we have all columns
                    incidents_data.append(cell_texts)
        
        return incidents_data
    except Exception as e:
        logging.error(f"Error extracting incidents: {str(e)}")
        return []

def save_incidents_to_csv(incidents_data, filename):
    """Save incident data to CSV file"""
    try:
        headers = ['Details', 'No.', 'Time', 'Type', 'Location', 'Location Desc.', 'Area']
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(headers)
            
            for row in incidents_data:
                writer.writerow(row)
        
        logging.info(f"Saved {len(incidents_data)} incidents to {filename}")
    except Exception as e:
        logging.error(f"Error saving to CSV: {str(e)}")

def incidents_to_json(incidents_data, center_code):
    """Convert incidents data to JSON format"""
    incidents = []
    for row in incidents_data:
        if len(row) >= 7:
            incident = {
                "id": row[1],  # No.
                "time": row[2],  # Time
                "type": row[3],  # Type
                "location": row[4],  # Location
                "location_desc": row[5],  # Location Desc.
                "area": row[6],  # Area
                "scraped_at": datetime.now().isoformat(),
                "center": center_code
            }
            incidents.append(incident)
    return incidents

def save_active_incidents(incidents_data, center_code):
    """Save active incidents as JSON (overwrites every time)"""
    try:
        os.makedirs('data', exist_ok=True)
        
        incidents = incidents_to_json(incidents_data, center_code)
        active_data = {
            "center": center_code,
            "center_name": COMMUNICATION_CENTERS.get(center_code, center_code),
            "last_updated": datetime.now().isoformat(),
            "incident_count": len(incidents),
            "incidents": incidents
        }
        
        with open('active_incidents.json', 'w', encoding='utf-8') as f:
            json.dump(active_data, f, indent=2, ensure_ascii=False)
        
        logging.info(f"Saved {len(incidents)} active incidents to active_incidents.json")
    except Exception as e:
        logging.error(f"Error saving active incidents: {str(e)}")

def send_email_alert(changes, center_code, center_name):
    """Send email alert for incident changes"""
    if not EMAIL_AVAILABLE:
        logging.info("Email notifications not available - skipping email alert")
        return False
    
    try:
        # Check if email notifications are enabled
        if not os.getenv('ENABLE_EMAIL_NOTIFICATIONS', 'false').lower() == 'true':
            logging.info("Email notifications disabled - skipping email alert")
            return False
        
        # Check if there are actual changes
        has_changes = (len(changes.get('new_incidents', [])) > 0 or 
                      len(changes.get('removed_incidents', [])) > 0)
        
        if not has_changes:
            logging.info("No changes detected - skipping email alert")
            return False
        
        # Initialize email notifier
        notifier = EmailNotifier()
        
        # Send alert
        success = notifier.send_incident_alert(changes, center_name, center_code)
        
        if success:
            logging.info(f"Email alert sent successfully for {center_name} center")
        else:
            logging.error("Failed to send email alert")
        
        return success
        
    except Exception as e:
        logging.error(f"Error sending email alert: {str(e)}")
        return False

def append_daily_incidents(incidents_data, center_code):
    """Append unique incidents to daily JSON file"""
    try:
        os.makedirs('data', exist_ok=True)
        
        today = datetime.now().strftime('%Y-%m-%d')
        daily_filename = f"data/{today}_incidents.json"
        
        # Load existing daily incidents
        existing_incidents = []
        if os.path.exists(daily_filename):
            with open(daily_filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing_incidents = data.get('incidents', [])
        
        # Get existing incident IDs
        existing_ids = {incident['id'] for incident in existing_incidents}
        
        # Convert new incidents to JSON
        new_incidents = incidents_to_json(incidents_data, center_code)
        
        # Filter out duplicates
        unique_new_incidents = [
            incident for incident in new_incidents 
            if incident['id'] not in existing_ids
        ]
        
        # Combine with existing incidents
        all_incidents = existing_incidents + unique_new_incidents
        
        # Save updated daily file
        daily_data = {
            "center": center_code,
            "center_name": COMMUNICATION_CENTERS.get(center_code, center_code),
            "date": today,
            "last_updated": datetime.now().isoformat(),
            "total_incidents": len(all_incidents),
            "new_incidents_today": len(unique_new_incidents),
            "incidents": all_incidents
        }
        
        with open(daily_filename, 'w', encoding='utf-8') as f:
            json.dump(daily_data, f, indent=2, ensure_ascii=False)
        
        logging.info(f"Appended {len(unique_new_incidents)} new incidents to {daily_filename}")
    except Exception as e:
        logging.error(f"Error appending daily incidents: {str(e)}")

def scrape_chp_incidents(center_code="BCCC", mode="local"):
    """Scrape CHP incidents for specified communication center"""
    driver = None
    try:
        center_name = COMMUNICATION_CENTERS.get(center_code, center_code)
        
        # Option 1: Use Safari (macOS built-in)
        if USE_SAFARI and mode == "local":
            print("Using Safari WebDriver...")
            driver = webdriver.Safari()
            logging.info("Safari WebDriver initialized")
        else:
            # Option 2: Use Chrome with proper ARM64 driver
            print("Using Chrome WebDriver...")
            if mode == "github_actions":
                # In GitHub Actions, use webdriver-manager for reliable ChromeDriver
                try:
                    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
                    logging.info("Chrome WebDriver initialized with webdriver-manager (GitHub Actions)")
                except Exception as e:
                    print(f"Webdriver-manager failed in GitHub Actions: {e}")
                    # Fallback to system ChromeDriver
                    driver = webdriver.Chrome()
                    logging.info("Chrome WebDriver initialized with system driver (GitHub Actions fallback)")
            else:
                try:
                    # Try using webdriver-manager first (auto-downloads correct driver)
                    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
                    logging.info("Chrome WebDriver initialized with webdriver-manager")
                except Exception as e:
                    print(f"Webdriver-manager failed: {e}")
                    print("Falling back to local chromedriver...")
                    # Fallback to local chromedriver
                    driver = webdriver.Chrome(service=Service(CHROMEDRIVER_PATH))
                    logging.info("Chrome WebDriver initialized with local driver")
        
        print("Webdriver loaded.")
        driver.get(URL)
        print("Page loaded.")

        # Wait for and select the Communications Center dropdown
        wait = WebDriverWait(driver, 10)
        center_dropdown = wait.until(EC.presence_of_element_located((By.ID, "ddlComCenter")))
        select = Select(center_dropdown)
        select.select_by_value(center_code)

        print(f"Selected '{center_name}' communications center ({center_code}).")
        
        # Click the OK button to submit the new selection
        ok_button = driver.find_element(By.ID, "btnCCGo")
        ok_button.click()
        print("Clicked OK button.")

        # Wait for page update AFTER selection
        time.sleep(5)

        page_source = driver.page_source

        # Extract incident data
        incidents_data = extract_incidents_from_table(driver)
        
        if incidents_data:
            # Compare with previous incidents (only in local mode)
            if mode == "local":
                global previous_incidents
                changes = compare_incidents(previous_incidents, incidents_data)
            
            # Save data based on mode
            if mode == "github_actions":
                # GitHub Actions mode: save JSON files and check for changes
                changes = compare_incidents(previous_incidents, incidents_data)
                
                save_active_incidents(incidents_data, center_code)
                append_daily_incidents(incidents_data, center_code)
                print(f"âœ… Saved {len(incidents_data)} incidents to JSON files")
                logging.info(f"Found {len(incidents_data)} incidents - saved to JSON files")
                
                # Send email alert if there are changes
                if changes and (len(changes.get('new_incidents', [])) > 0 or len(changes.get('removed_incidents', [])) > 0):
                    print("ğŸ“§ Sending email alert for changes...")
                    send_email_alert(changes, center_code, center_name)
                
                # Update previous incidents for next comparison
                previous_incidents = incidents_data.copy()
            else:
                # Local mode: save CSV and show diff
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_filename = f"chp_{center_code.lower()}_incidents_{timestamp}.csv"
                save_incidents_to_csv(incidents_data, csv_filename)
                
                # Print status and changes
                print(f"\nğŸ• [{datetime.now().strftime('%H:%M:%S')}] Scrape - {len(incidents_data)} incidents found")
                
                if previous_incidents is None:
                    print("ğŸ“Š Initial scrape - all incidents are new:")
                    print_incident_summary(incidents_data, "All Incidents")
                else:
                    # Check if there are any changes
                    has_changes = (len(changes["new_incidents"]) > 0 or 
                                 len(changes["removed_incidents"]) > 0)
                    
                    if has_changes:
                        print("ğŸ”„ CHANGES DETECTED:")
                        print_incident_summary(changes["new_incidents"], "New Incidents")
                        print_incident_summary(changes["removed_incidents"], "Resolved Incidents")
                        
                        # Send email alert for changes
                        print("ğŸ“§ Sending email alert for changes...")
                        send_email_alert(changes, center_code, center_name)
                    else:
                        print("âœ… No changes detected - same incidents as previous scrape")
                
                print(f"ğŸ’¾ Data saved to: {csv_filename}")
                logging.info(f"Found {len(incidents_data)} incidents - saved to {csv_filename}")
                
                # Update previous incidents for next comparison
                previous_incidents = incidents_data.copy()
        else:
            print(f"\nğŸ• [{datetime.now().strftime('%H:%M:%S')}] No incidents found for {center_name} center.")
            logging.info(f"No incidents found for {center_name} center.")
        
        return incidents_data
        
    except Exception as e:
        print("ERROR:", e)
        logging.error(f"Error: {str(e)}")
        return []
    finally:
        if driver is not None:
            driver.quit()
            print("Webdriver closed.")

def main():
    parser = argparse.ArgumentParser(description='CHP Traffic Incident Scraper')
    parser.add_argument('--center', '-c', 
                       choices=['BCCC', 'CCC', 'NCCC', 'SCCC'],
                       default='BCCC',
                       help='Communication center to scrape (default: BCCC)')
    parser.add_argument('--mode', '-m',
                       choices=['local', 'github_actions'],
                       default='local',
                       help='Run mode (default: local)')
    parser.add_argument('--iterations', '-i',
                       type=int,
                       default=10,
                       help='Number of iterations to run (default: 10)')
    parser.add_argument('--interval', '-t',
                       type=int,
                       default=60,
                       help='Interval between scrapes in seconds (default: 60)')
    
    args = parser.parse_args()
    
    center_name = COMMUNICATION_CENTERS.get(args.center, args.center)
    
    if args.mode == "github_actions":
        print(f"ğŸš¨ CHP {center_name} Traffic Monitor (GitHub Actions)")
        print(f"ğŸ“Š Scraping {args.center} center...")
        print("=" * 60)
        
        # Single scrape for GitHub Actions
        incidents = scrape_chp_incidents(args.center, args.mode)
        print(f"âœ… Scraping complete. Found {len(incidents)} incidents.")
    else:
        print(f"ğŸš¨ CHP {center_name} Traffic Monitor (Local)")
        print(f"ğŸ“Š Monitoring {args.center} center every {args.interval} seconds...")
        print("=" * 60)
        
        for i in range(args.iterations):
            print(f"\n--- Scraping iteration {i+1}/{args.iterations} ---")
            scrape_chp_incidents(args.center, args.mode)
            
            if i < args.iterations - 1:  # Don't sleep after the last iteration
                print(f"â° Waiting {args.interval} seconds until next scrape...")
                time.sleep(args.interval)
        
        print("\n" + "=" * 60)
        print("ğŸ Test run finished. Check 'chp_scraper_debug.log' for detailed logs.")

if __name__ == "__main__":
    main()
